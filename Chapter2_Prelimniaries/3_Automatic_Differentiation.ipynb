{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [],
      "source": "from mxnet import nd, autograd"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "y \u003d \n[28.]\n\u003cNDArray 1 @cpu(0)\u003e\n\n[ 0.  4.  8. 12.]\n\u003cNDArray 4 @cpu(0)\u003e\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\u0027\u0027\u0027\n    1. A Simple Example\n\u0027\u0027\u0027\nx \u003d nd.arange(4)\n# Tell an NDArray that we plan to store a gradient by invoking its attach_grad() method\nx.attach_grad()\n# Tell MXNet to build the graph explicitly\nwith autograd.record():\n    y \u003d 2*nd.dot(x, x)\n    print(\"y \u003d\", y)\ny.backward()\nprint(x.grad)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "dy/dx \u003d  \n[0. 2. 4. 6.]\n\u003cNDArray 4 @cpu(0)\u003e\ndv/du \u003d  \n[0. 2. 4. 6.]\n\u003cNDArray 4 @cpu(0)\u003e\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\u0027\u0027\u0027\n    2. Backward for Non-scalar Variable\n\u0027\u0027\u0027\n# When y is not scalar, the gradients could be high order tensor and complex to compute\n# As loss functions are ofter scalars, so MXNet will sum the element in y to get the new variable by default\n# y is a vector\nwith autograd.record():\n    y \u003d x * x\ny.backward()\nprint(\"dy/dx \u003d \", x.grad)\n\nu \u003d x.copy()\nu.attach_grad()\n# v is a scalar\nwith autograd.record():\n    v \u003d (u * u).sum()   \nv.backward()\nprint(\"dv/du \u003d \", u.grad)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Detach dz/dx \u003d  \n[0. 1. 4. 9.]\n\u003cNDArray 4 @cpu(0)\u003e\ndz/dx \u003d  \n[ 0.  3. 12. 27.]\n\u003cNDArray 4 @cpu(0)\u003e\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\u0027\u0027\u0027\n    3. Detach Computation\n\u0027\u0027\u0027\n# Move some parts of computations out of the computation graph\n# As code following, u will forget how y is computed and be treated as a constant\nwith autograd.record():\n    y \u003d x * x\n    u \u003d y.detach()\n    z \u003d u * x\nz.backward()\nprint(\"Detach dz/dx \u003d \", x.grad)\n\nwith autograd.record():\n    y \u003d x * x\n    z \u003d y * x\nz.backward()\nprint(\"dz/dx \u003d \", x.grad)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "dz/dx \n[1. 1. 1. 1.]\n\u003cNDArray 4 @cpu(0)\u003e\ndz/dy \n[0. 0. 0. 0.]\n\u003cNDArray 4 @cpu(0)\u003e\ndz/du \n[1. 1. 1. 1.]\n\u003cNDArray 4 @cpu(0)\u003e\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\u0027\u0027\u0027\n    4. Attach Gradients to Internal Variables\n\u0027\u0027\u0027\nx \u003d nd.arange(4)\nx.attach_grad()\n\ny \u003d nd.ones(4) * 2\ny.attach_grad()\nwith autograd.record():\n    u \u003d x * y\n    # You can add this line to see what will happen\n    u.attach_grad()  #implicitly run detach() and then u will forget how y is computed and be treated as a constant\n    z \u003d u + x\nz.backward()\nprint(\"dz/dx\", x.grad)\nprint(\"dz/dy\", y.grad)\nprint(\"dz/du\", u.grad)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "dz/dv \n[1. 1. 1. 1.]\n\u003cNDArray 4 @cpu(0)\u003e\ndz/dx \n[1. 1. 1. 1.]\n\u003cNDArray 4 @cpu(0)\u003e\ndz/dy \n[0. 0. 0. 0.]\n\u003cNDArray 4 @cpu(0)\u003e\ndz/dv \n[1. 1. 1. 1.]\n\u003cNDArray 4 @cpu(0)\u003e\ndz/dx \n[2. 2. 2. 2.]\n\u003cNDArray 4 @cpu(0)\u003e\ndz/dy \n[0. 1. 2. 3.]\n\u003cNDArray 4 @cpu(0)\u003e\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "\u0027\u0027\u0027\n    5. Head Gradients\n\u0027\u0027\u0027\ny \u003d nd.ones(4) * 2\ny.attach_grad()\nwith autograd.record():\n    u \u003d x * y\n    v \u003d u.detach()  # u still keeps the computation graph\n    v.attach_grad()\n    z \u003d v + x\nz.backward()\nprint(\"dz/dv\", v.grad)\nprint(\"dz/dx\", x.grad)\nprint(\"dz/dy\", y.grad)\n\n# Pass v.grad as the first term of u.backward(), then you would able to conduct the chain rule during back propagation\n# pass the first term as the head gradients to multiply both terms so that x.grad will contains  ùëëùëß/ùëëùë•  instead of  ùëëùë¢/ùëëùë•\nu.backward(v.grad)\nprint(\"dz/dv\", v.grad)\nprint(\"dz/dx\", x.grad)\nprint(\"dz/dy\", y.grad)\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}